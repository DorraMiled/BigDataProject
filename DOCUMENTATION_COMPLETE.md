# Documentation ComplÃ¨te - Projet E-Commerce Big Data avec ELK Stack

## ğŸ“‹ Table des MatiÃ¨res
1. [Vue d'ensemble du projet](#vue-densemble)
2. [Architecture technique](#architecture)
3. [Configuration initiale](#configuration-initiale)
4. [Configuration ELK Stack](#configuration-elk)
5. [Ingestion des donnÃ©es](#ingestion-donnees)
6. [ProblÃ¨mes rencontrÃ©s et solutions](#problemes-solutions)
7. [Visualisation dans Kibana](#visualisation-kibana)
8. [Commandes utiles](#commandes-utiles)

---

## ğŸ¯ Vue d'ensemble du projet {#vue-densemble}

### Objectif
CrÃ©er une plateforme d'analyse Big Data pour un site e-commerce utilisant la stack ELK (Elasticsearch, Logstash, Kibana) pour ingÃ©rer, stocker et visualiser les logs d'application.

### Technologies utilisÃ©es
- **Docker Desktop** : Conteneurisation des services
- **Elasticsearch 8.11.0** : Moteur de recherche et base de donnÃ©es NoSQL
- **Logstash 8.11.0** : Pipeline d'ingestion et transformation des donnÃ©es
- **Kibana 8.11.0** : Interface de visualisation et analyse
- **MongoDB 7.0** : Base de donnÃ©es principale
- **Redis 7-alpine** : Cache et gestion de sessions
- **Node.js 18** : Backend API
- **PowerShell** : Scripts d'automatisation

### DonnÃ©es traitÃ©es
Le projet traite 5 types de logs d'e-commerce :
- **Transactions** : Commandes, paiements (600 documents)
- **Errors** : Erreurs applicatives et API (597 documents)
- **Fraud** : DÃ©tection de fraude et bots (600 documents)
- **Performance** : Temps de rÃ©ponse API (600 documents)
- **User Behavior** : Comportement utilisateur (600 documents)

**Total : ~3000 documents ingÃ©rÃ©s**

---

## ğŸ—ï¸ Architecture technique {#architecture}

### SchÃ©ma de l'architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCKER COMPOSE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Elasticsearchâ”‚â—„â”€â”€â”€â”€â”€â”¤   Logstash   â”‚                   â”‚
â”‚  â”‚  Port 9200   â”‚      â”‚  Port 5000   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                     â”‚                            â”‚
â”‚         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚         â”‚              â”‚  Fichiers Logs â”‚                  â”‚
â”‚         â”‚              â”‚ - transactions â”‚                  â”‚
â”‚         â”‚              â”‚ - errors      â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”      â”‚ - fraud       â”‚                  â”‚
â”‚  â”‚    Kibana    â”‚      â”‚ - performance â”‚                  â”‚
â”‚  â”‚  Port 5601   â”‚      â”‚ - user_behaviorâ”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   MongoDB    â”‚      â”‚    Redis     â”‚                   â”‚
â”‚  â”‚  Port 27017  â”‚      â”‚  Port 6379   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚   Backend    â”‚                                          â”‚
â”‚  â”‚  Node.js API â”‚                                          â”‚
â”‚  â”‚  Port 3000   â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flux de donnÃ©es :
Fichiers (CSV/JSON) â†’ Logstash â†’ Elasticsearch â†’ Kibana
```

### RÃ©seau Docker
Tous les services communiquent via le rÃ©seau `ecommerce-network` (bridge driver).

### Volumes persistants
- `elasticsearch-data` : DonnÃ©es Elasticsearch
- `mongodb-data` : DonnÃ©es MongoDB
- `redis-data` : DonnÃ©es Redis
- `../ecommerce_logs` : Logs montÃ©s en lecture seule

---

## ğŸš€ Configuration initiale {#configuration-initiale}

### Ã‰tape 1 : CrÃ©ation du docker-compose.yml

**Fichier** : `docker-compose.yml`

**Contenu principal** :
- Services : elasticsearch, logstash, kibana, mongodb, redis, backend
- RÃ©seau : ecommerce-network
- Volumes : donnÃ©es persistantes

**Configuration Elasticsearch** :
```yaml
environment:
  - discovery.type=single-node
  - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  - xpack.security.enabled=false
ports:
  - "9200:9200"
  - "9300:9300"
```

**Raison** : Mode single-node pour dÃ©veloppement, sÃ©curitÃ© dÃ©sactivÃ©e pour simplifier.

**Configuration Logstash** :
```yaml
volumes:
  - ./elk-config/logstash/logstash-files.conf:/usr/share/logstash/pipeline/logstash.conf
  - ./elk-config/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml
  - ../ecommerce_logs:/usr/share/logstash/data
```

**Raison** : Monte le dossier de logs externe et la configuration personnalisÃ©e.

### Ã‰tape 2 : Configuration des fichiers ELK

#### A. elasticsearch.yml
**Emplacement** : `elk-config/elasticsearch/elasticsearch.yml`

```yaml
cluster.name: "ecommerce-cluster"
network.host: 0.0.0.0
xpack.security.enabled: false
discovery.type: single-node
```

**Explication** :
- `cluster.name` : Nom du cluster pour identification
- `network.host: 0.0.0.0` : Ã‰coute sur toutes les interfaces
- `xpack.security.enabled: false` : DÃ©sactive la sÃ©curitÃ© (dev uniquement)
- `discovery.type: single-node` : Mode nÅ“ud unique

#### B. logstash.yml
**Emplacement** : `elk-config/logstash/logstash.yml`

```yaml
http.host: "0.0.0.0"
xpack.monitoring.enabled: false
path.config: /usr/share/logstash/pipeline
```

**Explication** :
- Configure le port HTTP et le chemin des pipelines

#### C. kibana.yml
**Emplacement** : `elk-config/kibana/kibana.yml`

```yaml
server.host: "0.0.0.0"
elasticsearch.hosts: ["http://elasticsearch:9200"]
xpack.security.enabled: false
```

**Explication** :
- Connecte Kibana Ã  Elasticsearch via le nom de service Docker

### Ã‰tape 3 : Fichier .env

**Emplacement** : `.env`

```env
# MongoDB
MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=admin123
MONGODB_URI=mongodb://admin:admin123@mongodb:27017/ecommerce?authSource=admin

# Redis
REDIS_HOST=redis
REDIS_PORT=6379

# Elasticsearch
ELASTICSEARCH_HOST=http://elasticsearch:9200

# Logstash
LOGSTASH_HOST=logstash
LOGSTASH_TCP_PORT=5000
LOGSTASH_HTTP_PORT=8080
```

**Raison** : Centralise toutes les variables d'environnement pour faciliter la configuration.

---

## ğŸ”§ Configuration ELK Stack {#configuration-elk}

### Ã‰tape 4 : Configuration du pipeline Logstash

**Fichier** : `elk-config/logstash/logstash-files.conf`

#### Structure du pipeline Logstash

Le pipeline Logstash est divisÃ© en 3 sections :

##### 1. INPUT - Lecture des fichiers

**Fichiers NDJSON (Newline Delimited JSON)** :
```ruby
file {
  path => "/usr/share/logstash/data/transactions.ndjson"
  start_position => "beginning"
  sincedb_path => "/dev/null"
  codec => "json"
  type => "transaction"
}
```

**Explication** :
- `path` : Chemin du fichier dans le conteneur Docker
- `start_position => "beginning"` : Lit depuis le dÃ©but du fichier
- `sincedb_path => "/dev/null"` : DÃ©sactive le suivi de position (relit toujours depuis le dÃ©but)
- `codec => "json"` : Parser JSON pour NDJSON
- `type => "transaction"` : Tag pour identifier le type de log

**Fichiers CSV** :
```ruby
file {
  path => "/usr/share/logstash/data/transactions.csv"
  start_position => "beginning"
  sincedb_path => "/dev/null"
  type => "transaction"
}
```

**DiffÃ©rence** : Pas de codec, le parsing se fait dans la section filter.

##### 2. FILTER - Transformation des donnÃ©es

**DÃ©tection et parsing CSV** :
```ruby
if [log][file][path] =~ /\.csv$/ {
  if [type] == "transaction" {
    csv {
      source => "message"
      separator => ","
      columns => ["timestamp", "event", "order_id", "user", "amount", "method", "duration_ms"]
    }
    mutate {
      convert => {
        "amount" => "float"
        "duration_ms" => "integer"
      }
      remove_field => ["message"]
    }
  }
}
```

**Explication ligne par ligne** :
1. `if [log][file][path] =~ /\.csv$/` : DÃ©tecte les fichiers .csv via regex
2. `source => "message"` : Le contenu brut de la ligne CSV est dans le champ "message"
3. `separator => ","` : SÃ©pare les colonnes par virgule
4. `columns => [...]` : Nomme chaque colonne selon l'ordre dans le CSV
5. `convert` : Convertit les types de donnÃ©es (string â†’ float/integer)
6. `remove_field => ["message"]` : Supprime le champ message brut aprÃ¨s parsing

**Parsing du timestamp** :
```ruby
if [timestamp] {
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss'Z'"]
    target => "@timestamp"
  }
}
```

**Explication** :
- Convertit le champ `timestamp` (string) en objet date
- Stocke dans `@timestamp` (champ standard Elasticsearch pour le tri temporel)
- Supporte plusieurs formats de date

**DÃ©termination de l'index** :
```ruby
if [type] == "transaction" {
  mutate {
    add_field => { "[@metadata][index]" => "ecommerce-transactions" }
  }
}
```

**Explication** :
- `[@metadata][index]` : Variable temporaire non stockÃ©e dans Elasticsearch
- UtilisÃ©e dans la section output pour router vers le bon index

##### 3. OUTPUT - Envoi vers Elasticsearch

```ruby
output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index]}"
    document_type => "_doc"
  }
  stdout {
    codec => rubydebug
  }
}
```

**Explication** :
- `hosts` : URL d'Elasticsearch (nom de service Docker)
- `index => "%{[@metadata][index]}"` : Utilise l'index dÃ©terminÃ© dans filter
- `stdout` : Affiche dans les logs Docker pour debugging

---

## ğŸ“Š Ingestion des donnÃ©es {#ingestion-donnees}

### Ã‰tape 5 : PrÃ©paration des fichiers

#### Structure des fichiers source

**Emplacement** : `C:\Users\DELL\Desktop\3eme\Big Data\ecommerce_logs`

**Fichiers prÃ©sents** :
```
transactions.csv / transactions.json
errors.csv / errors.json
fraud.csv / fraud.json
performance.csv / performance.json
user_behavior.csv / user_behavior.json
```

#### Structure des fichiers CSV

**transactions.csv** :
```csv
timestamp,event,order_id,user,amount,method,duration_ms
2025-12-01T10:00:27Z,PAYMENT_FAILED,CMD90578,USR842,282.0,card,197
2025-12-01T10:00:46Z,PAYMENT_SUCCESS,CMD93968,USR320,76.77,paypal,139
```

**Colonnes** :
- `timestamp` : Date/heure au format ISO8601
- `event` : Type d'Ã©vÃ©nement (PAYMENT_SUCCESS, ORDER_CREATED, etc.)
- `order_id` : Identifiant de commande (format CMD#####)
- `user` : Identifiant utilisateur (format USR###)
- `amount` : Montant en euros (float)
- `method` : MÃ©thode de paiement (card, paypal, applepay)
- `duration_ms` : DurÃ©e en millisecondes (integer)

**errors.csv** :
```csv
timestamp,status,endpoint,message
2025-12-01T11:54:16Z,429,/api/products,Unauthorized
2025-12-01T11:54:49Z,500,/api/orders,Timeout
```

**fraud.csv** :
```csv
timestamp,event,user,ip,attempts
2025-12-01T17:32:05Z,BOT_DETECTED,USR695,192.168.151.200,9
```

**performance.csv** :
```csv
timestamp,event,endpoint,latency_ms,status
2025-12-01T15:43:33Z,API_RESPONSE,/api/products,170,503
```

**user_behavior.csv** :
```csv
timestamp,event,user,page
2025-12-01T13:51:03Z,USER_VISIT,USR574,/category/phones
```

#### Structure des fichiers JSON

**Format original** : JSON Array
```json
[
  {
    "timestamp": "2025-12-01T10:00:27Z",
    "event": "PAYMENT_FAILED",
    "order_id": "CMD90578",
    "user": "USR842",
    "amount": 282.0,
    "method": "card",
    "duration_ms": 197
  },
  {
    ...
  }
]
```

**ProblÃ¨me** : Logstash ne peut pas parser directement un JSON array multi-ligne.

**Solution** : Conversion en NDJSON (Newline Delimited JSON).

### Ã‰tape 6 : Script de conversion NDJSON

**Fichier** : `scripts/convert-to-ndjson.ps1`

```powershell
$sourcePath = "C:\Users\DELL\Desktop\3eme\Big Data\ecommerce_logs"
$files = @("transactions", "errors", "fraud", "performance", "user_behavior")

foreach ($file in $files) {
    $jsonFile = Join-Path $sourcePath "$file.json"
    $ndjsonFile = Join-Path $sourcePath "$file.ndjson"
    
    if (Test-Path $jsonFile) {
        # Lire le JSON array
        $jsonContent = Get-Content $jsonFile -Raw | ConvertFrom-Json
        
        # Ecrire chaque objet sur une ligne
        $lines = $jsonContent | ForEach-Object {
            ($_ | ConvertTo-Json -Compress)
        }
        [System.IO.File]::WriteAllLines($ndjsonFile, $lines)
    }
}
```

**Explication Ã©tape par Ã©tape** :

1. **DÃ©finition du chemin source** :
   ```powershell
   $sourcePath = "C:\Users\DELL\Desktop\3eme\Big Data\ecommerce_logs"
   ```

2. **Liste des fichiers Ã  traiter** :
   ```powershell
   $files = @("transactions", "errors", "fraud", "performance", "user_behavior")
   ```

3. **Boucle de traitement** :
   - `Get-Content -Raw` : Lit tout le fichier en une seule string
   - `ConvertFrom-Json` : Parse le JSON array en objets PowerShell
   - `ConvertTo-Json -Compress` : Convertit chaque objet en JSON sur une ligne
   - `[System.IO.File]::WriteAllLines()` : Ã‰crit sans BOM ni retours chariages Windows

**RÃ©sultat NDJSON** :
```json
{"timestamp":"2025-12-01T10:00:27Z","event":"PAYMENT_FAILED","order_id":"CMD90578","user":"USR842","amount":282.0,"method":"card","duration_ms":197}
{"timestamp":"2025-12-01T10:00:46Z","event":"PAYMENT_SUCCESS","order_id":"CMD93968","user":"USR320","amount":76.77,"method":"paypal","duration_ms":139}
```

**Avantage NDJSON** : Chaque ligne est un JSON valide, facile Ã  streamer et parser ligne par ligne.

**ExÃ©cution du script** :
```powershell
& "C:\Users\DELL\Desktop\3eme\Big Data\ECommerceBigData\scripts\convert-to-ndjson.ps1"
```

**RÃ©sultat** :
```
Conversion des fichiers JSON en NDJSON...
  Traitement: transactions.json
  OK: transactions.ndjson cree (300 objets)
  Traitement: errors.json
  OK: errors.ndjson cree (300 objets)
  ...
```

### Ã‰tape 7 : DÃ©marrage des services

```powershell
# Naviguer dans le dossier du projet
cd "C:\Users\DELL\Desktop\3eme\Big Data\ECommerceBigData"

# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier les conteneurs
docker-compose ps
```

**Sortie attendue** :
```
NAME            IMAGE                                         STATUS
elasticsearch   docker.elastic.co/elasticsearch:8.11.0        Up (healthy)
kibana          docker.elastic.co/kibana:8.11.0               Up
logstash        docker.elastic.co/logstash:8.11.0             Up
mongodb         mongo:7.0                                     Up
redis           redis:7-alpine                                Up
backend         ecommercebigdata-backend                      Up
```

### Ã‰tape 8 : VÃ©rification de l'ingestion

**Attendre 30 secondes** que Logstash traite les fichiers :
```powershell
Start-Sleep -Seconds 30
```

**VÃ©rifier les index crÃ©Ã©s** :
```powershell
Invoke-WebRequest -Uri "http://localhost:9200/_cat/indices?v" -UseBasicParsing
```

**RÃ©sultat attendu** :
```
health status index                   uuid                   pri rep docs.count
yellow open   ecommerce-transactions  YUhv4ra3RmSYGd4K_jwSBw   1   1        600
yellow open   ecommerce-errors        txB__AcSQ3mxwBoudNJF5g   1   1        597
yellow open   ecommerce-fraud         1hNLSEXrTlS7nQE7wBQUMQ   1   1        600
yellow open   ecommerce-performance   _aKbxFPBQ1mn7TAmXMRQPQ   1   1        600
yellow open   ecommerce-user-behavior lDcR0ek7SqO48qHBW0nCeA   1   1        600
```

**Explication du statut "yellow"** : Normal pour un cluster single-node (pas de rÃ©plication).

**VÃ©rifier le contenu d'un document** :
```powershell
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_search?size=1&pretty" -UseBasicParsing
```

**Document exemple** :
```json
{
  "_index": "ecommerce-transactions",
  "_id": "MldB3poBy4W42xw6Uymw",
  "_score": 1.0,
  "_source": {
    "event": "ORDER_CREATED",
    "user": "USR742",
    "type": "transaction",
    "amount": 260.69,
    "method": "paypal",
    "@timestamp": "2025-12-01T10:14:24.000Z",
    "duration_ms": 357,
    "order_id": "CMD93060"
  }
}
```

---

## ğŸ› ProblÃ¨mes rencontrÃ©s et solutions {#problemes-solutions}

### ProblÃ¨me 1 : Docker Desktop not running

**Erreur** :
```
unable to get image: error during connect: open //./pipe/dockerDesktopLinuxEngine
```

**Cause** : Docker Desktop n'Ã©tait pas dÃ©marrÃ©.

**Solution** :
1. Ouvrir Docker Desktop depuis le menu DÃ©marrer
2. Attendre que l'icÃ´ne Docker dans la barre des tÃ¢ches soit stable
3. VÃ©rifier avec `docker --version`
4. Relancer `docker-compose up -d`

### ProblÃ¨me 2 : Version obsolÃ¨te dans docker-compose.yml

**Avertissement** :
```
the attribute `version` is obsolete, it will be ignored
```

**Solution** : Suppression de la ligne `version: '3.8'` du fichier docker-compose.yml.

**Explication** : Les versions rÃ©centes de Docker Compose n'ont plus besoin de cette ligne.

### ProblÃ¨me 3 : Parsing JSON multi-ligne Ã©choue

**Erreur dans les logs Logstash** :
```
"tags" => ["_jsonparsefailure"]
"message" => "  {"
```

**Cause** : Les fichiers JSON sont des arrays multi-lignes, pas du NDJSON.

**Solution** : CrÃ©ation du script `convert-to-ndjson.ps1` pour convertir.

**Avant** :
```json
[
  {
    "field": "value"
  }
]
```

**AprÃ¨s (NDJSON)** :
```json
{"field":"value"}
```

### ProblÃ¨me 4 : Fichiers CSV non parsÃ©s

**SymptÃ´me** : Les documents dans Elasticsearch contiennent seulement un champ `message` avec toute la ligne CSV.

**Exemple de document incorrect** :
```json
{
  "message": "2025-12-01T10:09:51Z,PAYMENT_SUCCESS,CMD92637,USR354,64.2,applepay,571\r",
  "type": "transaction"
}
```

**Cause** : Le filtre CSV ne dÃ©tectait pas correctement les fichiers CSV.

**Code problÃ©matique** :
```ruby
if [path] =~ "\.csv$" {  # âŒ Le champ [path] n'existe pas
  csv { ... }
}
```

**Solution** : Utilisation du bon chemin de champ et ajout de `source => "message"` :
```ruby
if [log][file][path] =~ /\.csv$/ {  # âœ… Bon chemin
  csv {
    source => "message"  # âœ… Parser le champ message
    separator => ","
    columns => [...]
  }
  mutate {
    remove_field => ["message"]  # âœ… Supprimer aprÃ¨s parsing
  }
}
```

**RÃ©sultat aprÃ¨s correction** :
```json
{
  "event": "PAYMENT_SUCCESS",
  "order_id": "CMD92637",
  "user": "USR354",
  "amount": 64.2,
  "method": "applepay",
  "duration_ms": 571,
  "@timestamp": "2025-12-01T10:09:51.000Z"
}
```

### ProblÃ¨me 5 : Double configuration Logstash

**SymptÃ´me** : Deux fichiers de pipeline sont chargÃ©s simultanÃ©ment.

**VÃ©rification** :
```powershell
docker exec -it logstash ls -la /usr/share/logstash/pipeline/
# RÃ©sultat: logstash.conf ET logstash-files.conf
```

**Cause** : Le docker-compose.yml montait deux fichiers :
```yaml
volumes:
  - ./elk-config/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
  - ./elk-config/logstash/logstash-files.conf:/usr/share/logstash/pipeline/logstash-files.conf
```

**Solution** : Garder seulement un fichier de configuration :
```yaml
volumes:
  - ./elk-config/logstash/logstash-files.conf:/usr/share/logstash/pipeline/logstash.conf
```

### ProblÃ¨me 6 : Encoding Windows (CRLF vs LF)

**SymptÃ´me** : Les fichiers NDJSON contiennent `\r` (retour chariot Windows).

**Cause** : `Out-File` en PowerShell utilise l'encoding Windows par dÃ©faut.

**Code problÃ©matique** :
```powershell
$lines | Out-File -FilePath $ndjsonFile -Encoding UTF8  # âŒ Ajoute CRLF
```

**Solution** : Utilisation de la mÃ©thode .NET :
```powershell
[System.IO.File]::WriteAllLines($ndjsonFile, $lines)  # âœ… Utilise LF uniquement
```

### ProblÃ¨me 7 : Index vides dans Kibana

**SymptÃ´me** : Les index existent mais apparaissent vides dans Kibana Discover.

**Cause** : Le filtre de temps (Time Range) dans Kibana Ã©tait mal configurÃ©.

**Solution** :
1. Dans Kibana Discover, cliquer sur le calendrier en haut Ã  droite
2. SÃ©lectionner "Last 7 days" ou "Last 30 days"
3. Ou dÃ©finir une plage personnalisÃ©e incluant le 1er dÃ©cembre 2025

**Explication** : Les logs ont des timestamps du 1er dÃ©cembre 2025, si Kibana affiche "Last 15 minutes", aucune donnÃ©e n'apparaÃ®t.

---

## ğŸ“Š Visualisation dans Kibana {#visualisation-kibana}

### Ã‰tape 9 : Configuration des Index Patterns

**AccÃ¨s** : http://localhost:5601

#### CrÃ©ation d'un Index Pattern

1. **Navigation** :
   - Cliquer sur le menu hamburger (â˜°) en haut Ã  gauche
   - Aller dans **Management** â†’ **Stack Management**
   - Dans le menu gauche, section **Kibana**, cliquer sur **Data Views** (ou **Index Patterns**)

2. **CrÃ©ation** :
   - Cliquer sur **Create data view**
   - Name: `Transactions E-Commerce`
   - Index pattern: `ecommerce-transactions`
   - Timestamp field: `@timestamp`
   - Cliquer sur **Save data view to Kibana**

3. **RÃ©pÃ©ter pour chaque type** :
   - `ecommerce-errors`
   - `ecommerce-fraud`
   - `ecommerce-performance`
   - `ecommerce-user-behavior`

4. **Pattern global (optionnel)** :
   - Index pattern: `ecommerce-*`
   - Permet de chercher dans tous les index simultanÃ©ment

#### VÃ©rification dans Discover

1. Aller dans **Analytics** â†’ **Discover**
2. SÃ©lectionner un index pattern dans le menu dÃ©roulant
3. Ajuster le Time Range (ex: Last 30 days)
4. Explorer les champs dans la colonne gauche

**Champs disponibles par index** :

**Transactions** :
- `event` : Type d'Ã©vÃ©nement
- `order_id` : ID commande
- `user` : ID utilisateur
- `amount` : Montant
- `method` : MÃ©thode paiement
- `duration_ms` : DurÃ©e traitement

**Errors** :
- `status` : Code HTTP
- `endpoint` : URL API
- `error_message` : Message d'erreur
- `timestamp` : Date/heure

**Fraud** :
- `event` : Type de fraude
- `user` : Utilisateur suspect
- `ip` : Adresse IP
- `attempts` : Nombre tentatives

**Performance** :
- `event` : Type Ã©vÃ©nement
- `endpoint` : URL
- `latency_ms` : Latence
- `status` : Code retour

**User Behavior** :
- `event` : Action utilisateur
- `user` : ID utilisateur
- `page` : Page visitÃ©e

### Exemples de requÃªtes KQL

**KQL** (Kibana Query Language) permet de filtrer les donnÃ©es.

#### Transactions Ã©chouÃ©es
```kql
event: "PAYMENT_FAILED"
```

#### Transactions par mÃ©thode de paiement
```kql
method: "paypal"
```

#### Montants Ã©levÃ©s
```kql
amount > 200
```

#### Erreurs 500
```kql
status: 500
```

#### Fraude dÃ©tectÃ©e
```kql
event: "BOT_DETECTED"
```

#### Performance lente
```kql
latency_ms > 500
```

#### Utilisateur spÃ©cifique
```kql
user: "USR123"
```

#### Combinaison
```kql
event: "PAYMENT_SUCCESS" AND method: "card" AND amount > 100
```

### CrÃ©ation de visualisations

#### 1. Graphique en secteurs (Pie Chart) - RÃ©partition des paiements

1. Aller dans **Analytics** â†’ **Visualize Library**
2. Cliquer sur **Create visualization**
3. SÃ©lectionner **Pie**
4. Choisir l'index `ecommerce-transactions`
5. Configuration :
   - **Slice by** : Terms
   - **Field** : method.keyword
   - **Size** : 10
6. Cliquer sur **Update**
7. Sauvegarder la visualisation

#### 2. Graphique linÃ©aire - Transactions au fil du temps

1. **Visualize Library** â†’ **Create visualization** â†’ **Line**
2. Index : `ecommerce-transactions`
3. Configuration :
   - **Vertical axis** : Count
   - **Horizontal axis** : @timestamp (Date Histogram)
   - **Minimum interval** : 1 hour
4. Filtres optionnels : `event: "PAYMENT_SUCCESS"`

#### 3. Tableau de donnÃ©es - Top utilisateurs

1. **Visualize Library** â†’ **Create visualization** â†’ **Data table**
2. Index : `ecommerce-transactions`
3. Configuration :
   - **Rows** : Terms of user.keyword (Top 10)
   - **Metrics** :
     - Count
     - Sum of amount
     - Average of duration_ms

#### 4. MÃ©trique - Montant total

1. **Visualize Library** â†’ **Create visualization** â†’ **Metric**
2. Index : `ecommerce-transactions`
3. Configuration :
   - **Metric** : Sum
   - **Field** : amount
4. Ajouter un filtre : `event: "PAYMENT_SUCCESS"`

#### 5. Graphique Ã  barres - Erreurs par endpoint

1. **Visualize Library** â†’ **Create visualization** â†’ **Bar vertical**
2. Index : `ecommerce-errors`
3. Configuration :
   - **Y-axis** : Count
   - **X-axis** : Terms of endpoint.keyword
   - **Split series** : Terms of status

### CrÃ©ation d'un Dashboard

1. Aller dans **Analytics** â†’ **Dashboard**
2. Cliquer sur **Create dashboard**
3. Cliquer sur **Add** pour ajouter des visualisations
4. SÃ©lectionner les visualisations crÃ©Ã©es prÃ©cÃ©demment
5. Organiser avec drag & drop
6. Ajuster les tailles
7. Sauvegarder le dashboard

**Exemple de structure de dashboard** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Dashboard E-Commerce - Vue d'ensemble    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Montant Totalâ”‚ Nb Commandes â”‚ Tx de succÃ¨s      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transactions au fil du temps (Line Chart)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RÃ©partition paiement â”‚ Top 10 utilisateurs      â”‚
â”‚ (Pie Chart)          â”‚ (Table)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Erreurs par endpoint (Bar Chart)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Commandes utiles {#commandes-utiles}

### Gestion Docker Compose

```powershell
# DÃ©marrer tous les services
docker-compose up -d

# DÃ©marrer un service spÃ©cifique
docker-compose up -d elasticsearch

# ArrÃªter tous les services
docker-compose down

# ArrÃªter et supprimer les volumes (ATTENTION: perte de donnÃ©es)
docker-compose down -v

# Voir les logs de tous les services
docker-compose logs

# Suivre les logs en temps rÃ©el
docker-compose logs -f

# Logs d'un service spÃ©cifique
docker-compose logs -f logstash

# Voir les derniÃ¨res 50 lignes
docker-compose logs --tail=50 logstash

# Voir le statut des conteneurs
docker-compose ps

# RedÃ©marrer un service
docker-compose restart logstash

# Rebuild et redÃ©marrer
docker-compose up -d --build backend
```

### Commandes Elasticsearch

```powershell
# VÃ©rifier la santÃ© du cluster
Invoke-WebRequest -Uri "http://localhost:9200/_cluster/health?pretty" -UseBasicParsing

# Lister tous les index
Invoke-WebRequest -Uri "http://localhost:9200/_cat/indices?v" -UseBasicParsing

# Voir les index triÃ©s
Invoke-WebRequest -Uri "http://localhost:9200/_cat/indices?v&s=index" -UseBasicParsing

# Compter les documents d'un index
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_count" -UseBasicParsing

# Chercher dans un index (1 document)
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_search?size=1&pretty" -UseBasicParsing

# Chercher avec une requÃªte
$body = '{"query":{"match":{"event":"PAYMENT_SUCCESS"}}}'
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_search?pretty" -Method Post -Body $body -ContentType "application/json"

# Supprimer un index
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions" -Method Delete -UseBasicParsing

# Supprimer tous les documents (mais garder l'index)
$body = '{"query":{"match_all":{}}}'
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_delete_by_query" -Method Post -Body $body -ContentType "application/json"

# Voir le mapping d'un index
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_mapping?pretty" -UseBasicParsing

# Voir les statistiques d'un index
Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-transactions/_stats?pretty" -UseBasicParsing
```

### Commandes Docker exec

```powershell
# AccÃ©der au shell d'un conteneur
docker exec -it elasticsearch bash
docker exec -it logstash bash
docker exec -it kibana bash

# Voir les fichiers dans Logstash
docker exec -it logstash ls -la /usr/share/logstash/data/

# Voir les pipelines Logstash
docker exec -it logstash ls -la /usr/share/logstash/pipeline/

# Voir la configuration Elasticsearch
docker exec -it elasticsearch cat /usr/share/elasticsearch/config/elasticsearch.yml

# Tester la connectivitÃ©
docker exec -it logstash curl http://elasticsearch:9200

# Voir les processus
docker exec -it elasticsearch ps aux
```

### Scripts PowerShell

```powershell
# Conversion NDJSON
& "C:\Users\DELL\Desktop\3eme\Big Data\ECommerceBigData\scripts\convert-to-ndjson.ps1"

# VÃ©rifier tous les index avec dÃ©tails
$indices = @("transactions", "errors", "fraud", "performance", "user-behavior")
foreach ($idx in $indices) {
    Write-Host "`n=== ecommerce-$idx ===" -ForegroundColor Cyan
    $r = Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-$idx/_search?size=1" -UseBasicParsing
    $json = $r.Content | ConvertFrom-Json
    Write-Host "Documents: $($json.hits.total.value)"
}

# Supprimer tous les index ecommerce
@("transactions", "errors", "fraud", "performance", "user-behavior") | ForEach-Object {
    Invoke-WebRequest -Uri "http://localhost:9200/ecommerce-$_" -Method Delete -UseBasicParsing -ErrorAction SilentlyContinue
    Write-Host "Supprime: ecommerce-$_"
}
```

### Debugging

```powershell
# VÃ©rifier que Docker Desktop fonctionne
docker --version
docker ps

# VÃ©rifier la connectivitÃ© rÃ©seau
docker network ls
docker network inspect ecommercebigdata_ecommerce-network

# VÃ©rifier les volumes
docker volume ls
docker volume inspect ecommercebigdata_elasticsearch-data

# Voir l'utilisation des ressources
docker stats

# Voir les logs du dÃ©marrage
docker-compose up

# Nettoyer les ressources inutilisÃ©es
docker system prune -a --volumes
```

---

## ğŸ“ˆ RÃ©sultats finaux

### Statistiques d'ingestion

```
Index                    Documents  Taille
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ecommerce-transactions   600        221.2kb
ecommerce-errors         597        211.2kb
ecommerce-fraud          600        183.2kb
ecommerce-performance    600        143.4kb
ecommerce-user-behavior  600        160.0kb
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                    2,997      ~919kb
```

### Performance

- **Temps d'ingestion** : ~30 secondes pour ~3000 documents
- **Temps de recherche** : < 10ms pour les requÃªtes simples
- **MÃ©moire Elasticsearch** : 512MB heap
- **MÃ©moire Logstash** : 256MB heap

### Architecture finale

```
Fichiers sources (CSV + JSON)
    â†“
Script PowerShell (conversion NDJSON)
    â†“
Volume Docker (../ecommerce_logs)
    â†“
Logstash (parsing + transformation)
    â†“
Elasticsearch (stockage + indexation)
    â†“
Kibana (visualisation + analyse)
```

---

## ğŸ“ Concepts clÃ©s appris

### 1. Architecture ELK

- **E**lasticsearch : Base de donnÃ©es NoSQL orientÃ©e recherche
- **L**ogstash : ETL (Extract, Transform, Load) pour donnÃ©es
- **K**ibana : Interface de visualisation

### 2. Docker Compose

- Orchestration multi-conteneurs
- RÃ©seaux Docker
- Volumes persistants
- Health checks

### 3. Logstash Pipelines

- Input plugins (file, tcp, http)
- Filter plugins (csv, json, date, mutate)
- Output plugins (elasticsearch, stdout)
- Conditions et patterns regex

### 4. Elasticsearch

- Index et documents
- Mapping automatique
- API REST
- RequÃªtes et agrÃ©gations

### 5. Kibana

- Index Patterns / Data Views
- Discover (exploration)
- Visualize Library (graphiques)
- Dashboard (tableaux de bord)
- KQL (Kibana Query Language)

---

## ğŸš€ AmÃ©liorations possibles

### Court terme

1. **SÃ©curitÃ©** :
   - Activer x-pack security
   - CrÃ©er des utilisateurs avec rÃ´les
   - Utiliser HTTPS

2. **Performance** :
   - Augmenter la heap Elasticsearch
   - Ajouter des index lifecycle policies
   - Optimiser les mappings

3. **Monitoring** :
   - Ajouter Metricbeat
   - Configurer les alertes
   - Dashboard de monitoring

### Long terme

1. **ScalabilitÃ©** :
   - Cluster Elasticsearch multi-nodes
   - Load balancer pour Kibana
   - Multiple Logstash instances

2. **Backend API** :
   - Routes REST pour ingestion en temps rÃ©el
   - WebSocket pour streaming
   - Rate limiting

3. **Frontend Angular** :
   - Dashboard personnalisÃ©
   - IntÃ©gration Elasticsearch JS
   - Graphiques temps rÃ©el

4. **Machine Learning** :
   - DÃ©tection d'anomalies
   - PrÃ©diction de fraude
   - Recommandations produits

---

## ğŸ“š Ressources

### Documentation officielle

- [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
- [Kibana Documentation](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

### Fichiers de configuration

```
ECommerceBigData/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ KIBANA_GUIDE.md
â”œâ”€â”€ INGESTION.md
â”œâ”€â”€ elk-config/
â”‚   â”œâ”€â”€ elasticsearch/
â”‚   â”‚   â””â”€â”€ elasticsearch.yml
â”‚   â”œâ”€â”€ logstash/
â”‚   â”‚   â”œâ”€â”€ logstash.yml
â”‚   â”‚   â””â”€â”€ logstash-files.conf
â”‚   â””â”€â”€ kibana/
â”‚       â””â”€â”€ kibana.yml
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ server.js
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ scripts/
â””â”€â”€ scripts/
    â””â”€â”€ convert-to-ndjson.ps1
```

---

## âœ… Checklist de dÃ©marrage

- [ ] Docker Desktop installÃ© et dÃ©marrÃ©
- [ ] Fichiers de logs dans `ecommerce_logs/`
- [ ] Script NDJSON exÃ©cutÃ©
- [ ] `docker-compose up -d` exÃ©cutÃ©
- [ ] Attendre 30-60 secondes
- [ ] VÃ©rifier les index : `http://localhost:9200/_cat/indices?v`
- [ ] AccÃ©der Ã  Kibana : `http://localhost:5601`
- [ ] CrÃ©er les Index Patterns
- [ ] Explorer dans Discover
- [ ] CrÃ©er des visualisations
- [ ] Assembler un Dashboard

---

## ğŸ†˜ Support et dÃ©pannage

### ProblÃ¨me : Elasticsearch ne dÃ©marre pas

**VÃ©rifications** :
```powershell
docker-compose logs elasticsearch
```

**Solutions** :
- Augmenter la mÃ©moire Docker (Settings > Resources > Memory : 4GB+)
- VÃ©rifier les ports disponibles : `netstat -ano | findstr "9200"`

### ProblÃ¨me : Aucun document dans les index

**VÃ©rifications** :
```powershell
docker-compose logs logstash
docker exec -it logstash ls -la /usr/share/logstash/data/
```

**Solutions** :
- VÃ©rifier que les fichiers NDJSON existent
- VÃ©rifier la configuration Logstash
- RedÃ©marrer Logstash : `docker-compose restart logstash`

### ProblÃ¨me : Kibana n'affiche pas les donnÃ©es

**Solutions** :
- Ajuster le Time Range (Last 30 days)
- RafraÃ®chir l'Index Pattern (Management > Index Patterns > Refresh)
- VÃ©rifier que @timestamp est configurÃ©

---

**Fin de la documentation**

*Projet rÃ©alisÃ© le 2 dÃ©cembre 2025*
*Stack : Elasticsearch 8.11.0, Logstash 8.11.0, Kibana 8.11.0*
*Plateforme : Docker Desktop sur Windows*
